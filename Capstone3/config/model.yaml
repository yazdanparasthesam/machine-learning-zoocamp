model:
  name: distilbert-base-uncased
  num_classes: 2
  pretrained: true

training:
  batch_size: 16
  epochs: 3
  learning_rate: 2e-5
  optimizer: adamw
  loss: cross_entropy

data:
  raw_dir: data/raw
  processed_dir: data/processed
  train_file: data/processed/train.csv
  val_file: data/processed/val.csv
  test_file: data/processed/test.csv
  max_length: 512
  train_split: 0.7
  val_split: 0.15
  test_split: 0.15
  random_seed: 42

runtime:
  device: auto
  num_workers: 2
  pin_memory: true
