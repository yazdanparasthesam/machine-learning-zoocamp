{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Detection â€“ Capstone3 Project\n",
    "\n",
    "## Objective\n",
    "The goal of this project is to build a machine learning/NLP model\n",
    "to detect whether a news article is **fake** or **real**.\n",
    "\n",
    "This notebook focuses on:\n",
    "- Dataset exploration (EDA for text)\n",
    "- Data quality checks\n",
    "- Baseline model training\n",
    "- Model evaluation\n",
    "\n",
    "Production deployment, monitoring, and API orchestration\n",
    "are implemented separately in scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# For reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Structure Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data/processed\"\n",
    "\n",
    "for split in [\"train.csv\", \"val.csv\", \"test.csv\"]:\n",
    "    path = os.path.join(DATA_DIR, split)\n",
    "    df = pd.read_csv(path)\n",
    "    print(f\"\\n{split.upper()}\")\n",
    "    print(f\"Number of samples: {len(df)}\")\n",
    "    print(\"Class distribution:\")\n",
    "    print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Sample Texts (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_samples(df, label, n=5):\n",
    "    print(f\"\\nSamples from class '{label}':\\n\")\n",
    "    samples = df[df['label']==label]['text'].sample(n)\n",
    "    for i, text in enumerate(samples):\n",
    "        print(f\"{i+1}. {text[:500]}...\\n\")  # show first 500 chars\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\n",
    "show_samples(train_df, \"fake\")\n",
    "show_samples(train_df, \"real\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Transformations / Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # remove URLs\n",
    "    text = re.sub(r\"\\d+\", \"\", text)      # remove numbers\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "train_df['text_clean'] = train_df['text'].apply(clean_text)\n",
    "val_df = pd.read_csv(os.path.join(DATA_DIR, \"val.csv\"))\n",
    "val_df['text_clean'] = val_df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1,2))\n",
    "X_train = vectorizer.fit_transform(train_df['text_clean'])\n",
    "X_val = vectorizer.transform(val_df['text_clean'])\n",
    "\n",
    "y_train = train_df['label'].map({\"fake\":0, \"real\":1}).values\n",
    "y_val = val_df['label'].map({\"fake\":0, \"real\":1}).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Dataset & DataLoader (Optional, for later transformer use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X.todense(), dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_ds = NewsDataset(X_train, y_train)\n",
    "val_ds = NewsDataset(X_val, y_val)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = LogisticRegression(max_iter=1000)\n",
    "baseline_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = baseline_model.predict(X_val)\n",
    "\n",
    "print(classification_report(y_val, y_pred, target_names=[\"fake\", \"real\"]))\n",
    "\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[\"fake\", \"real\"], yticklabels=[\"fake\", \"real\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Quick PyTorch Feedforward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small FFNN for demonstration\n",
    "input_dim = X_train.shape[1]\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "ffnn_model = FFNN(input_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(ffnn_model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop (Few Epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(3):\n",
    "    ffnn_model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_dl:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = ffnn_model(X_batch)\n",
    "        loss = criterion(logits, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_dl):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffnn_model.eval()\n",
    "all_preds, all_targets = [], []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in val_dl:\n",
    "        X_batch = X_batch.to(device)\n",
    "        logits = ffnn_model(X_batch)\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_targets.extend(y_batch.numpy())\n",
    "\n",
    "print(classification_report(all_targets, all_preds, target_names=[\"fake\", \"real\"]))\n",
    "\n",
    "cm = confusion_matrix(all_targets, all_preds)\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Greens\",\n",
    "            xticklabels=[\"fake\", \"real\"], yticklabels=[\"fake\", \"real\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix - PyTorch Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "- Dataset is moderately balanced between fake and real classes.\n",
    "- TF-IDF + Logistic Regression baseline performs reasonably well.\n",
    "- Misclassifications often occur on very short or ambiguous articles.\n",
    "- PyTorch feedforward network shows similar performance; transformer upgrade can improve metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This notebook validated the feasibility of text-based fake news detection using both:\n",
    "- Classical ML (TF-IDF + Logistic Regression)\n",
    "- Lightweight neural networks (PyTorch feedforward)\n",
    "\n",
    "The system is ready for:\n",
    "- Full training scripts (`train.py`)\n",
    "- API-based inference (`predict.py`)\n",
    "- Deployment, monitoring, and production scaling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
